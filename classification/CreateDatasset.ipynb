{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f260a5",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e4f2b",
   "metadata": {},
   "source": [
    "## Extract Claims\n",
    "* The `.ann` files are in the folders `data/p` and `data/n` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d41c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phrases = {\n",
    "    'interpretation': [],\n",
    "    'evaluation_rational': [],\n",
    "    'evaluation_emotional': [],\n",
    "    'agreement': [],\n",
    "    'disagreement': []\n",
    "}\n",
    "\n",
    "for fname in os.listdir('data/p'):\n",
    "    cur_types = {}\n",
    "    cur_phrases = {}\n",
    "    with open(f'data/p/{fname}') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                _, identifier, info = parts[1].split(' ')\n",
    "                cur_types[identifier] = info.replace('\\n', '')\n",
    "            else:\n",
    "                identifier = parts[0]\n",
    "                info = parts[1]\n",
    "                phrase = parts[2]\n",
    "                phrase_type, _, _ = info.split(' ')\n",
    "                if phrase_type == 'claim':\n",
    "                    cur_phrases[identifier] = phrase.replace('\\n', '')\n",
    "    \n",
    "    for identifier in cur_phrases.keys():\n",
    "        phrases[cur_types[identifier]].append(cur_phrases[identifier])\n",
    "\n",
    "for fname in os.listdir('data/n'):\n",
    "    cur_types = {}\n",
    "    cur_phrases = {}\n",
    "    with open(f'data/n/{fname}') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                _, identifier, info = parts[1].split(' ')\n",
    "                cur_types[identifier] = info.replace('\\n', '')\n",
    "            else:\n",
    "                identifier = parts[0]\n",
    "                info = parts[1]\n",
    "                phrase = parts[2]\n",
    "                phrase_type, _, _ = info.split(' ')\n",
    "                if phrase_type == 'claim':\n",
    "                    cur_phrases[identifier] = phrase.replace('\\n', '')\n",
    "    \n",
    "    for identifier in cur_phrases.keys():\n",
    "        phrases[cur_types[identifier]].append(cur_phrases[identifier])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c00b5b",
   "metadata": {},
   "source": [
    "## Generate Indices for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_idcs(n_samples, split_train=0.7, split_val=0.2, split_test=0.1):\n",
    "    assert np.isclose(split_train + split_val + split_test, 1), 'Splits must add up to 1.'\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    train_end = int(n_samples*split_train)\n",
    "    val_end = train_end + int(n_samples*split_val)\n",
    "    return perm[:train_end], perm[train_end:val_end], perm[val_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "idcs = list(map(lambda k: train_val_test_idcs(len(phrases[k])), phrases.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735e228",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186353d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "val = {}\n",
    "test = {}\n",
    "for i, key in enumerate(phrases.keys()):\n",
    "    train[key] = np.array(phrases[key])[idcs[i][0]]\n",
    "    val[key] = np.array(phrases[key])[idcs[i][1]]\n",
    "    test[key] = np.array(phrases[key])[idcs[i][2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a469b",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8c68b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('data_train.p', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "with open('data_val.p', 'wb') as f:\n",
    "    pickle.dump(val, f)\n",
    "    \n",
    "with open('data_test.p', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d3ffd",
   "metadata": {},
   "source": [
    "### Save in a sklearn compatible way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([])\n",
    "y = np.array([])\n",
    "for cls, key in enumerate(val.keys()):\n",
    "    X = np.append(X, val[key])\n",
    "    y = np.append(y, np.repeat(cls, len(val[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddfb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('dataset.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ac2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([])\n",
    "y = np.array([])\n",
    "for cls, key in enumerate(test.keys()):\n",
    "    X = np.append(X, test[key])\n",
    "    y = np.append(y, np.repeat(cls, len(test[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('dataset_test.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396c4b0",
   "metadata": {},
   "source": [
    "### Generate Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = np.zeros(len(phrases.keys()))\n",
    "for i, key in enumerate(phrases.keys()):\n",
    "    cnts[i] = len(phrases[key])\n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1844296",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(range(5), cnts)\n",
    "plt.xticks(range(5), labels=phrases.keys(), rotation=15)\n",
    "plt.savefig('class_distributuin.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ad88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
